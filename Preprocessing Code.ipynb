{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from itertools import combinations\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import contractions\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Read the JSON Lines file\n",
    "with open('ML Datasets/ML Dataset 1/News_Category_Dataset_v3.json', 'r') as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "# Add commas between JSON objects and format as a list\n",
    "with open('output.json', 'w') as outfile:\n",
    "    outfile.write('[\\n')  # Start of JSON array\n",
    "    for i, line in enumerate(lines):\n",
    "        # Strip newline characters and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # Add comma at the end of each line except the last one\n",
    "        if i < len(lines) - 1:\n",
    "            outfile.write(line + ',\\n')\n",
    "        else:\n",
    "            outfile.write(line + '\\n')\n",
    "    outfile.write(']\\n')  # End of JSON array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                link  \\\n",
      "0  https://www.huffpost.com/entry/covid-boosters-...   \n",
      "1  https://www.huffpost.com/entry/american-airlin...   \n",
      "2  https://www.huffpost.com/entry/funniest-tweets...   \n",
      "3  https://www.huffpost.com/entry/funniest-parent...   \n",
      "4  https://www.huffpost.com/entry/amy-cooper-lose...   \n",
      "\n",
      "                                            headline   category  \\\n",
      "0  Over 4 Million Americans Roll Up Sleeves For O...  U.S. NEWS   \n",
      "1  American Airlines Flyer Charged, Banned For Li...  U.S. NEWS   \n",
      "2  23 Of The Funniest Tweets About Cats And Dogs ...     COMEDY   \n",
      "3  The Funniest Tweets From Parents This Week (Se...  PARENTING   \n",
      "4  Woman Who Called Cops On Black Bird-Watcher Lo...  U.S. NEWS   \n",
      "\n",
      "                                   short_description               authors  \\\n",
      "0  Health experts said it is too early to predict...  Carla K. Johnson, AP   \n",
      "1  He was subdued by passengers and crew when he ...        Mary Papenfuss   \n",
      "2  \"Until you have a dog you don't understand wha...         Elyse Wanshel   \n",
      "3  \"Accidentally put grown-up toothpaste on my to...      Caroline Bologna   \n",
      "4  Amy Cooper accused investment firm Franklin Te...        Nina Golgowski   \n",
      "\n",
      "        date  \n",
      "0 2022-09-23  \n",
      "1 2022-09-23  \n",
      "2 2022-09-23  \n",
      "3 2022-09-23  \n",
      "4 2022-09-22  \n",
      "(209527, 6)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"output.json\")\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209521, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(209521, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['link'] != \"\"]\n",
    "df = df[df['headline'] != \"\"]\n",
    "df = df[df['category'] != \"\"]\n",
    "print(df.shape)\n",
    "\n",
    "df_baljyot = df.iloc[30000:,:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# Applying tqdm to panda dataframes to observe processing progress for large datasets\n",
    "tqdm.pandas()\n",
    "\n",
    "# Function to get the text of the article from a given URL\n",
    "def fetch_article_text(url):\n",
    "    try:\n",
    "        # Sending a request to the URL\n",
    "        response = requests.get(url, timeout = 10)\n",
    "        \n",
    "        # Checking to see if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            # Parsing the HTML content\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Extracting the main article text\n",
    "            article_text = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "            return article_text\n",
    "        else:\n",
    "            print(f\"Request failed: {url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: Error fetching article from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parallel function to fetch article texts for a given DataFrame chunk\n",
    "def fetch_article_chunk(index_url_pair):\n",
    "    idx, url = index_url_pair\n",
    "    return (idx, fetch_article_text(url))\n",
    "\n",
    "# Fetching the article's text for each link in parallel\n",
    "df_from_csv = pd.read_csv('ML Datasets/ML Dataset 1/debjit.csv')\n",
    "\n",
    "# Prepare a placeholder NumPy array to store the article texts in order\n",
    "article_texts = np.empty(len(df_baljyot), dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 312/2500 [00:40<06:24,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed: https://www.huffingtonpost.com/entry/exploring-the-gay-straight-divide_us_5965263ce4b09be68c0055ca\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 428/2500 [00:57<03:22, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception: Error fetching article from https://www.huffingtonpost.comhttps://www.theguardian.com/lifeandstyle/2017/jul/10/workin-it-how-female-drag-queens-are-causing-a-scene: HTTPSConnectionPool(host='www.huffingtonpost.comhttps', port=443): Max retries exceeded with url: /www.theguardian.com/lifeandstyle/2017/jul/10/workin-it-how-female-drag-queens-are-causing-a-scene (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000208AEA085F0>: Failed to resolve 'www.huffingtonpost.comhttps' ([Errno 11001] getaddrinfo failed)\"))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 556/2500 [01:14<04:19,  7.49it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[0;32m     13\u001b[0m         article_texts[idx] \u001b[38;5;241m=\u001b[39m text\n\u001b[1;32m---> 15\u001b[0m \u001b[43mprocess_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# process_chunk(2500, 5000)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# process_chunk(5000, 7500)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# process_chunk(7500, 10000)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# process_chunk(10000, 12500)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# process_chunk(12500, len(df_from_csv))\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mprocess_chunk\u001b[1;34m(start_idx, end_idx)\u001b[0m\n\u001b[0;32m      6\u001b[0m chunk \u001b[38;5;241m=\u001b[39m index_url_pairs[start_idx:end_idx]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Map the function to the index-url pairs for the current chunk\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m     futures \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfetch_article_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assign the article texts to the correct index positions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, text \u001b[38;5;129;01min\u001b[39;00m futures:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 355\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the index-url pairs to pass to the executor (maintains original index)\n",
    "index_url_pairs = list(df_baljyot.iloc[:, 0].items())  # (index, URL)\n",
    "\n",
    "# Function to process a chunk of URLs\n",
    "def process_chunk(start_idx, end_idx):\n",
    "    chunk = index_url_pairs[start_idx:end_idx]\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Map the function to the index-url pairs for the current chunk\n",
    "        futures = list(tqdm(executor.map(fetch_article_chunk, chunk), total=len(chunk)))\n",
    "    \n",
    "    # Assign the article texts to the correct index positions\n",
    "    for idx, text in futures:\n",
    "        article_texts[idx] = text\n",
    "\n",
    "process_chunk(0, 2500)\n",
    "# process_chunk(2500, 5000)\n",
    "# process_chunk(5000, 7500)\n",
    "# process_chunk(7500, 10000)\n",
    "# process_chunk(10000, 12500)\n",
    "# process_chunk(12500, len(df_from_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.huffingtonpost.com/entry/marni-for-hm_us_5b9b6d8ee4b03a1dcc77773f'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding the fetched article texts back to the DataFrame\n",
    "df_baljyot['article_text'] = article_texts\n",
    "\n",
    "# Saving the modified DataFrame to a new CSV\n",
    "df_baljyot.to_csv('ML Datasets/ML Dataset 1/baljyot_mod.csv', index=False)\n",
    "\n",
    "# df_from_csv.head()\n",
    "df_from_csv.iloc[0,0]\n",
    "# print(article_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Author, travel writer and mom, lover of la dolce vita Stop the presses. I am officially a door buster.  I will go to great lengths for style. And, am often mortified (post acquisition) at what I will/would/will endure to hunt and gather the objects of my affection. Case in point? The activities surrounding the March 8 sale of uber expensive Marni for H&M. Now, I have been bowing down to the temple of Marni since the early 90\\'s. Designer Consuelo Castiglione\\'s upbeat prints and signature slouchy fit are my version of wardrobe perfection. But, in order to score discount designer duds, one must wade through the degradation that punctuates a fast fashion shopping experience. Having spent decades conquering sample sales in NYC and LA, I knew that my chances at nabbing my wish list would double if I had a partner. So, I enlisted my trusty assistant, Alaina, to coordinate an attack plan. In order to snap up fabulous fast fashion, one must physically go to the store. Very early. You see, these pieces are made in limited edition so they sell out within hours of their release into the wild. We planned for Alaina to get in line first so I could get my kids ready for school. But, after monitoring the sale on Twitter, I could see the merch was already selling out across the country. So, I bribed my husband to stay home and get kids organized. Then -- hair still in sleep ponytail -- I hightailed by bus to Michigan Avenue by 8 a.m.  I was not alone. There were swarms of women drooling like lionesses over fresh meat. Humorless guard keeping shoppers at bay, Amy and Alaina caffeinating and sea of Marni/H&M hopefuls Though we were told on the phone that we could shop at 8 a.m., this was not to be. Instead, we were given color coded wristbands with the stipulated shopping time. So, though I arrived at 8:15 a.m., I could not shop-or even browse-the collection until 10:05. Now, I am the type who can finagle an upgrade at the Four Seasons from standard room to suite. I routinely wrangle my way to First Class on American Airlines. I have sat in the front row at YSL couture in Paris (when issued a seat in the nosebleed section) and can essentially talk my way into almost any situation. But, I could not get H&M personnel (I even called the PR department) to budge on my designated shopping time. In fact, the dour security at H&M felt positively Storm Trooper-esque. Shoppers were herded up by bull horn and lined up behind stanchions. Rules of the sale were shouted out: 15 minute time limit to shop. No re-entering the sale area from dressing rooms. Limit of two of a single item could be purchased. Wrist bands were checked with military precision three times prior to entering the sale area. And, the final indignity, a DJ was on site to blast obnoxious dance music in what felt like an attempt to maximize frayed nerves. When I approached the gloriously accessorized mannequins to examine the designs, guards formed a human blockade and commanded me to retreat. Frustrated, I moved to the front of the store and watched as the hoarding commenced. Forget mesh shopping bags. Sly fashion fiends were scooping up multiples of everything and loading their goods onto garment racks. I watched with mounting anxiety as the fabulous floral chokers and cuffs were depleted before my eyes. Then the mound of sleek platform shoes began to dwindle. Style stress is a breeding ground for new BFFs. Friendships were forged. Shopping strategies were hatched. Coffees were inhaled. I actually found an adorable model for an upcoming tv segment I was producing.  We have to wait until 10.05?!  As 10:00 rolled around, we were all on high alert protecting our terrain in the 10:05 line. A woman started elbowing her way to the front. I alerted my newfound buddies and we headed her off and pelted her with evil eyes. At long last, our time was called. We were commanded to raise our left arms to display our wrist bands as we marched forth to the promised (fashion) land. I charged into the pit (to the lyrics of \"Staying Alive,\" ironically) stockpiling my bags with anything that may work. The glam accessories were gone. But, we stuffed our bags with the color blocked dresses, sporty black nylon windbreakers and African themed tops.  Next up? The moment of truth. The dressing room. After all of the waiting, everyone wanted their efforts to be endorsed with a prize purchase. Not everyone was rewarded. Size 10s were shimmying into size 6. Size 6 was swimming in size 12. And, some experienced the sad truth: what looks great on a mannequin does not work on every body type.  Amy with her spoils and post-shopping elixirs to rebalance  Luckily, I scored. A lot of what I selected was parfait. But, I was pissed that some of my wish list had not been on the sales floor. So, I waited. Some of those polka dot ¾ coats and cardigans were most likely sequestered behind dressing room doors. My assistant kept her eyes trained on the \"didn\\'t work\" rack while I sat waiting to pounce on discarded merch left by my fellow shoppers. Some women were willing to barter. A jacquard top for a print Capri? A size 4 shirtdress for platform wedges? Finally, at 11 a.m., I had enough. With a pounding head and bulging shopping bags, I ambled out of the store. It was an experience, yes. But, not one that I am not likely to repeat again.  Please visit my FIVE MINUTE FIXES blog for fast and fabulous style tips! By entering your email and clicking Sign Up, you\\'re agreeing to let us send you customized marketing messages about us and our advertising partners. You are also agreeing to our Terms of Service and Privacy Policy.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_csv.iloc[0,df_from_csv.shape[1]-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [04:29<00:00,  3.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Function to remove the punctuation marks from the given text\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    # Returning the cleaned text for further processing\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# Function to remove the stopwords from the given text\n",
    "def remove_stopwords(text):\n",
    "\n",
    "    # Defining a set of English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing the stop words from the given text based on the above created English stopword set\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Combining tokens back into a single string\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "\n",
    "    # Returning the cleaned text for further processing\n",
    "    return cleaned_text\n",
    "\n",
    "# Function to lemmatize the given text\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    # Initializing the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lemmatizing each token of the given text to break them down to their root forms\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Combining the lemmatized tokens back into a single string abd returning it for further processing\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Function to remove URLs from the given text\n",
    "def remove_urls(text):\n",
    "\n",
    "    # Defining the URL pattern to remove from the given text\n",
    "    url_pattern = r'(?:(?:http|https|ftp|ftps|sftp|file|mailto|tel|ws|wss)://|www\\.)[a-zA-Z0-9-]+(?:\\.[a-zA-Z0-9-]+)+[^\\s]*'\n",
    "\n",
    "    # Returning the new text with all URLs removed for further processing\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "# Function to remove the html tags (if any) from the given text\n",
    "def remove_html_tags(text):\n",
    "\n",
    "    # Creating a BeautifulSoup object and parsing the text with html.parser\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "\n",
    "    # Getting the text without HTML tags and returning it for further processing\n",
    "    return soup.get_text()\n",
    "\n",
    "# Function to expand contractions (for example: \"don't\" to \"do not\")\n",
    "def expand_contractions(text):\n",
    "\n",
    "    # Returning the new text with expanded words for further processing\n",
    "    return contractions.fix(text)\n",
    "\n",
    "# Function to remove the numbers from the given text\n",
    "def remove_numbers(text):\n",
    "    \n",
    "    # Returning the new text with numbers removed for further processing\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# Function to keep only valid English words in the given text\n",
    "def remove_invalid_words(text):\n",
    "\n",
    "    # Tokenizing the given text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # # Get the list of valid English words\n",
    "    word_list = set(words.words())\n",
    "\n",
    "    # Checking validity for each word\n",
    "    valid_words = [word for word in tokens if word in word_list]\n",
    "\n",
    "    # Returning the list of valid English words extracted from the given text for further processing\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "#  Preprocessing function for cleaning the article's text for further processing\n",
    "def preprocess_text(text, to_remove_stopwords = True, lemmatize = True):\n",
    "\n",
    "    # Converting all the article's text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Removing all the urls from the article's text (keeping this before removing punctuation to avoid problems)\n",
    "    text = remove_urls(text)\n",
    "\n",
    "    # Removing all the punctuation marks from the article's text\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    text = remove_html_tags(text) # Removing the html tags (if any)\n",
    "    text = expand_contractions(text) # Expanding contractions (For example: \"don't\" to \"do not\")\n",
    "    text = remove_numbers(text) # Removing numbers from the text\n",
    "\n",
    "    text = remove_invalid_words(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    if(to_remove_stopwords):\n",
    "        text = remove_stopwords(text)\n",
    "    \n",
    "    # Lemmatizing the words to break them to their root meanings\n",
    "    if(lemmatize):\n",
    "        text = lemmatize_text(text)    \n",
    "\n",
    "    # Returning the final clean text\n",
    "    return text\n",
    "\n",
    "# Test Example for the article text preprocessing function\n",
    "# text = \"Hi! My name is abc. What is your name? my number is 123, https://www.youtube.com/ dlfjk kdkd dkdkdkd  <break> don't hadn't won't  \"\n",
    "# text2 = \"using https://www.google.com/ as an example\"\n",
    "\n",
    "# ptext = preprocess_text(text)\n",
    "# ptext2 = preprocess_text(text2)\n",
    "# print(ptext)\n",
    "# print(ptext2)\n",
    "\n",
    "# Main implementation for the rows of the article dataframe\n",
    "df['cleaned_text'] = df['article_text'].progress_apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('ML Datasets/Ml Dataset 1/debjit_mod.csv')\n",
    "df2=pd.read_csv('ML Datasets/Ml Dataset 1/vijval_mod.csv')\n",
    "df3=pd.read_csv('ML Datasets/Ml Dataset 1/jaleel_mod.csv')\n",
    "df4=pd.read_csv('ML Datasets/Ml Dataset 1/baljyot_mod.csv')\n",
    "df_combined = pd.concat([df1, df2,df3, df4], ignore_index=True)\n",
    "\n",
    "df_combined.to_csv(\"ML Datasets/Ml Dataset 1/combined_extracted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('ML Datasets/ML Dataset 1/combined_without_nans.csv')\n",
    "df[\"weighted_text\"]= 10*(df['cleaned_headline']+\" \")+ 4*(df['cleaned_short_description']+\" \")+df[\"cleaned_article_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>abroad</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>...</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youngest</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zealand</th>\n",
       "      <th>zero</th>\n",
       "      <th>zika</th>\n",
       "      <th>zimmerman</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.037671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01715</td>\n",
       "      <td>0.024961</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaron  abandoned  abc  ability  able  abortion  abroad  absence  absolute  \\\n",
       "0    0.0   0.000000  0.0      0.0   0.0       0.0     0.0      0.0       0.0   \n",
       "1    0.0   0.037671  0.0      0.0   0.0       0.0     0.0      0.0       0.0   \n",
       "2    0.0   0.000000  0.0      0.0   0.0       0.0     0.0      0.0       0.0   \n",
       "3    0.0   0.000000  0.0      0.0   0.0       0.0     0.0      0.0       0.0   \n",
       "4    0.0   0.000000  0.0      0.0   0.0       0.0     0.0      0.0       0.0   \n",
       "\n",
       "   absolutely  ...    young   younger  youngest  youth  youtube  zealand  \\\n",
       "0         0.0  ...  0.00000  0.000000       0.0    0.0      0.0      0.0   \n",
       "1         0.0  ...  0.00000  0.000000       0.0    0.0      0.0      0.0   \n",
       "2         0.0  ...  0.01715  0.024961       0.0    0.0      0.0      0.0   \n",
       "3         0.0  ...  0.00000  0.000000       0.0    0.0      0.0      0.0   \n",
       "4         0.0  ...  0.00000  0.000000       0.0    0.0      0.0      0.0   \n",
       "\n",
       "   zero  zika  zimmerman  zone  \n",
       "0   0.0   0.0        0.0   0.0  \n",
       "1   0.0   0.0        0.0   0.0  \n",
       "2   0.0   0.0        0.0   0.0  \n",
       "3   0.0   0.0        0.0   0.0  \n",
       "4   0.0   0.0        0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OkapiBM25\n",
    "\n",
    "# # Preprocess the entire corpus\n",
    "# cleaned_texts = [preprocess_text(doc) for doc in df.iloc[:,-1]]\n",
    "\n",
    "# # Tokenize the cleaned corpus\n",
    "# tokenized_texts = [word_tokenize(doc) for doc in cleaned_texts]\n",
    "\n",
    "# # Create a BM25 model\n",
    "# bm25_model = BM25Okapi(tokenized_texts)\n",
    "\n",
    "# # Create vocabulary from the entire corpus\n",
    "# vocabulary = list(set(word for doc in tokenized_texts for word in doc))\n",
    "\n",
    "# # Create a BM25 score matrix\n",
    "# bm25_score_matrix = []\n",
    "\n",
    "# for word in vocabulary:\n",
    "#     # Get BM25 scores for each document for the current word\n",
    "#     score = bm25_model.get_scores(word)\n",
    "#     bm25_score_matrix.append(score)\n",
    "\n",
    "# # Convert the BM25 score matrix to a DataFrame and transpose to get a 2x5 shape\n",
    "# bm25_features = pd.DataFrame(np.array(bm25_score_matrix).T, columns=vocabulary)\n",
    "\n",
    "# bm25_features_df = bm25_features\n",
    "\n",
    "# bm25_features_df['id'] = [i for i in range(1000)]\n",
    "# print(bm25_features_df)\n",
    "\n",
    "### TF-IDF Feature Extraction\n",
    "\n",
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer_tf = TfidfVectorizer(max_features=5000) \n",
    "\n",
    "# Fit and transform the cleaned text, with a progress bar\n",
    "# Using a custom approach since TfidfVectorizer does not support tqdm directly\n",
    "tfidf_matrix = vectorizer_tf.fit_transform(df['weighted_text'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame for better visualization (optional)\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer_tf.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the TF-IDF DataFrame\n",
    "tfidf_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Apply LDA\u001b[39;00m\n\u001b[0;32m     11\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m lda_features \u001b[38;5;241m=\u001b[39m \u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_term_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m lda_features_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(lda_features, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lda_model\u001b[38;5;241m.\u001b[39mn_components)])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(lda_features_df)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:671\u001b[0m, in \u001b[0;36mLatentDirichletAllocation.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    663\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_em_step(\n\u001b[0;32m    664\u001b[0m             X[idx_slice, :],\n\u001b[0;32m    665\u001b[0m             total_samples\u001b[38;5;241m=\u001b[39mn_samples,\n\u001b[0;32m    666\u001b[0m             batch_update\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    667\u001b[0m             parallel\u001b[38;5;241m=\u001b[39mparallel,\n\u001b[0;32m    668\u001b[0m         )\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     \u001b[38;5;66;03m# batch update\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_em_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;66;03m# check perplexity\u001b[39;00m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evaluate_every \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:522\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._em_step\u001b[1;34m(self, X, total_samples, batch_update, parallel)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"EM update for 1 iteration.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03mupdate `_component` by batch VB or online VB.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[38;5;124;03m    Unnormalized document topic distribution.\u001b[39;00m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# E-step\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m _, suff_stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[38;5;66;03m# M-step\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_update:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:465\u001b[0m, in \u001b[0;36mLatentDirichletAllocation._e_step\u001b[1;34m(self, X, cal_sstats, random_init, parallel)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m parallel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 465\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_update_doc_distribution\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp_dirichlet_component_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdoc_topic_prior_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_doc_update_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean_change_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcal_sstats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx_slice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgen_even_slices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# merge result\u001b[39;00m\n\u001b[0;32m    479\u001b[0m doc_topics, sstats_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_lda.py:144\u001b[0m, in \u001b[0;36m_update_doc_distribution\u001b[1;34m(X, exp_topic_word_distr, doc_topic_prior, max_doc_update_iter, mean_change_tol, cal_sstats, random_state)\u001b[0m\n\u001b[0;32m    140\u001b[0m last_d \u001b[38;5;241m=\u001b[39m doc_topic_d\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# exp(E[log(theta_{dk})]) * exp(E[log(beta_{dw})]).\u001b[39;00m\n\u001b[1;32m--> 144\u001b[0m norm_phi \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_doc_topic_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_topic_word_d\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m eps\n\u001b[0;32m    146\u001b[0m doc_topic_d \u001b[38;5;241m=\u001b[39m exp_doc_topic_d \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(cnts \u001b[38;5;241m/\u001b[39m norm_phi, exp_topic_word_d\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# Note: adds doc_topic_prior to doc_topic_d, in-place.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### LDA\n",
    "\n",
    "# Sample preprocessed text data\n",
    "documents =list(df['weighted_text'])\n",
    "\n",
    "# Step 1: Create a document-term matrix (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Step 2: Apply LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=15, random_state=42)\n",
    "lda_features = lda_model.fit(doc_term_matrix)\n",
    "\n",
    "lda_features_df = pd.DataFrame(lda_features, columns=[f'Component {i}' for i in range(lda_model.n_components)])\n",
    "\n",
    "print(lda_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Component 0  Component 1  Component 2  Component 3  Component 4  \\\n",
      "0       0.000000          0.0          0.0          0.0          0.0   \n",
      "1       0.006889          0.0          0.0          0.0          0.0   \n",
      "2       0.013777          0.0          0.0          0.0          0.0   \n",
      "3       0.000000          0.0          0.0          0.0          0.0   \n",
      "4       0.027555          0.0          0.0          0.0          0.0   \n",
      "..           ...          ...          ...          ...          ...   \n",
      "995     6.854252          0.0          0.0          0.0          0.0   \n",
      "996     6.858834          0.0          0.0          0.0          0.0   \n",
      "997     6.868029          0.0          0.0          0.0          0.0   \n",
      "998     6.874918          0.0          0.0          0.0          0.0   \n",
      "999     6.881807          0.0          0.0          0.0          0.0   \n",
      "\n",
      "     Component 5  Component 6  Component 7  Component 8  Component 9  ...  \\\n",
      "0            0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "1            0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "2            0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "3            0.0          0.0          0.0          0.0     0.030877  ...   \n",
      "4            0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "..           ...          ...          ...          ...          ...  ...   \n",
      "995          0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "996          0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "997          0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "998          0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "999          0.0          0.0          0.0          0.0     0.000000  ...   \n",
      "\n",
      "     Component 91  Component 92  Component 93  Component 94  Component 95  \\\n",
      "0             0.0           0.0           0.0           0.0           0.0   \n",
      "1             0.0           0.0           0.0           0.0           0.0   \n",
      "2             0.0           0.0           0.0           0.0           0.0   \n",
      "3             0.0           0.0           0.0           0.0           0.0   \n",
      "4             0.0           0.0           0.0           0.0           0.0   \n",
      "..            ...           ...           ...           ...           ...   \n",
      "995           0.0           0.0           0.0           0.0           0.0   \n",
      "996           0.0           0.0           0.0           0.0           0.0   \n",
      "997           0.0           0.0           0.0           0.0           0.0   \n",
      "998           0.0           0.0           0.0           0.0           0.0   \n",
      "999           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "     Component 96  Component 97  Component 98  Component 99   id  \n",
      "0             0.0           0.0      0.000000           0.0    0  \n",
      "1             0.0           0.0      0.000000           0.0    1  \n",
      "2             0.0           0.0      0.000000           0.0    2  \n",
      "3             0.0           0.0      0.046971           0.0    3  \n",
      "4             0.0           0.0      0.000000           0.0    4  \n",
      "..            ...           ...           ...           ...  ...  \n",
      "995           0.0           0.0      0.000000           0.0  995  \n",
      "996           0.0           0.0      0.000000           0.0  996  \n",
      "997           0.0           0.0      0.000000           0.0  997  \n",
      "998           0.0           0.0      0.000000           0.0  998  \n",
      "999           0.0           0.0      0.000000           0.0  999  \n",
      "\n",
      "[1000 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the number of topics for NMF\n",
    "n_topics = 100\n",
    "\n",
    "# Initialize NMF model\n",
    "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
    "\n",
    "# Fit the NMF model to the combined matrix\n",
    "nmf_features = nmf_model.fit_transform(doc_term_matrix)\n",
    "\n",
    "# Create a DataFrame for the NMF features\n",
    "nmf_features_df = pd.DataFrame(nmf_features, columns=[f'Component {i}' for i in range(n_topics)])\n",
    "\n",
    "print(nmf_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   heartiness  markedly  scare  brilliant  spiraled  moppy  contradiction  \\\n",
      "0         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "1         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "2         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "3         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "4         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "\n",
      "   map  retired  need  ...  Component 90  Component 91  Component 92  \\\n",
      "0  0.0      0.0   0.0  ...      0.010000      0.010000      0.010000   \n",
      "1  0.0      0.0   0.0  ...      0.005000      0.005000      0.005000   \n",
      "2  0.0      0.0   0.0  ...      0.003333      0.003333      0.003333   \n",
      "3  0.0      0.0   0.0  ...      0.000003      0.000003      0.000003   \n",
      "4  0.0      0.0   0.0  ...      0.002000      0.002000      0.002000   \n",
      "\n",
      "   Component 93  Component 94  Component 95  Component 96  Component 97  \\\n",
      "0      0.010000      0.010000      0.010000      0.010000      0.010000   \n",
      "1      0.005000      0.005000      0.005000      0.005000      0.005000   \n",
      "2      0.003333      0.003333      0.003333      0.003333      0.003333   \n",
      "3      0.000003      0.000003      0.000003      0.000003      0.000003   \n",
      "4      0.002000      0.002000      0.002000      0.002000      0.002000   \n",
      "\n",
      "   Component 98  Component 99  \n",
      "0      0.010000      0.010000  \n",
      "1      0.005000      0.005000  \n",
      "2      0.003333      0.003333  \n",
      "3      0.000003      0.000003  \n",
      "4      0.002000      0.002000  \n",
      "\n",
      "[5 rows x 13199 columns]\n",
      "   heartiness  markedly  scare  brilliant  spiraled  moppy  contradiction  \\\n",
      "0         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "1         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "2         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "3         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "4         0.0       0.0    0.0        0.0       0.0    0.0            0.0   \n",
      "\n",
      "   map  retired  need  ...  Component 90  Component 91  Component 92  \\\n",
      "0  0.0      0.0   0.0  ...           0.0           0.0           0.0   \n",
      "1  0.0      0.0   0.0  ...           0.0           0.0           0.0   \n",
      "2  0.0      0.0   0.0  ...           0.0           0.0           0.0   \n",
      "3  0.0      0.0   0.0  ...           0.0           0.0           0.0   \n",
      "4  0.0      0.0   0.0  ...           0.0           0.0           0.0   \n",
      "\n",
      "   Component 93  Component 94  Component 95  Component 96  Component 97  \\\n",
      "0           0.0           0.0           0.0           0.0           0.0   \n",
      "1           0.0           0.0           0.0           0.0           0.0   \n",
      "2           0.0           0.0           0.0           0.0           0.0   \n",
      "3           0.0           0.0           0.0           0.0           0.0   \n",
      "4           0.0           0.0           0.0           0.0           0.0   \n",
      "\n",
      "   Component 98  Component 99  \n",
      "0      0.000000           0.0  \n",
      "1      0.000000           0.0  \n",
      "2      0.000000           0.0  \n",
      "3      0.046971           0.0  \n",
      "4      0.000000           0.0  \n",
      "\n",
      "[5 rows x 13199 columns]\n"
     ]
    }
   ],
   "source": [
    "### Merging the POS Tagging, TF-IDF, and LDA featurs into a single dataframe\n",
    "\n",
    "# pos_and_tfidf_features_df = pd.DataFrame()\n",
    "\n",
    "# # Merging POS tagging features with the TF-IDF features\n",
    "# for index, row in tfidf_features_df.iterrows():\n",
    "#     # print(index)\n",
    "#     for i in range(0,len(row)-1):\n",
    "#         word = tfidf_features_df.columns[i]\n",
    "#         # print(word)\n",
    "#         row[i] = {\n",
    "#             'tfidf_score': row[i],\n",
    "#             'pos_tag': pos_tagging_features[index].get(word, 'NULL')\n",
    "#         }\n",
    "#         # print(row[i])\n",
    "#     # print(row)\n",
    "#     pos_and_tfidf_features_df = pos_and_tfidf_features_df._append(row,ignore_index = True)\n",
    "\n",
    "# print(pos_and_tfidf_features_df)\n",
    "\n",
    "# Merging the LDA features with above combined dataframe\n",
    "final_features_df = pd.merge(bm25_features_df, lda_features_df, on='id', how='left')\n",
    "\n",
    "# Displaying the combined features DataFrame\n",
    "print(final_features_df.head())\n",
    "\n",
    "# Merging the LDA features with above combined dataframe\n",
    "final_features_df_2 = pd.merge(bm25_features_df, nmf_features_df, on='id', how='left')\n",
    "\n",
    "# Displaying the combined features DataFrame\n",
    "print(final_features_df_2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Component 0</th>\n",
       "      <th>Component 1</th>\n",
       "      <th>Component 2</th>\n",
       "      <th>Component 3</th>\n",
       "      <th>Component 4</th>\n",
       "      <th>Component 5</th>\n",
       "      <th>Component 6</th>\n",
       "      <th>Component 7</th>\n",
       "      <th>Component 8</th>\n",
       "      <th>Component 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Component 94</th>\n",
       "      <th>Component 95</th>\n",
       "      <th>Component 96</th>\n",
       "      <th>Component 97</th>\n",
       "      <th>Component 98</th>\n",
       "      <th>Component 99</th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Over 4 Million Americans Roll Up Sleeves For O...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>American Airlines Flyer Charged, Banned For Li...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>23 Of The Funniest Tweets About Cats And Dogs ...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>The Funniest Tweets From Parents This Week (Se...</td>\n",
       "      <td>2022-09-23</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.027555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>Woman Who Called Cops On Black Bird-Watcher Lo...</td>\n",
       "      <td>2022-09-22</td>\n",
       "      <td>U.S. NEWS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Component 0  Component 1  Component 2  Component 3  Component 4  \\\n",
       "0     0.000000          0.0          0.0          0.0          0.0   \n",
       "1     0.006889          0.0          0.0          0.0          0.0   \n",
       "2     0.013777          0.0          0.0          0.0          0.0   \n",
       "3     0.000000          0.0          0.0          0.0          0.0   \n",
       "4     0.027555          0.0          0.0          0.0          0.0   \n",
       "\n",
       "   Component 5  Component 6  Component 7  Component 8  Component 9  ...  \\\n",
       "0          0.0          0.0          0.0          0.0     0.000000  ...   \n",
       "1          0.0          0.0          0.0          0.0     0.000000  ...   \n",
       "2          0.0          0.0          0.0          0.0     0.000000  ...   \n",
       "3          0.0          0.0          0.0          0.0     0.030877  ...   \n",
       "4          0.0          0.0          0.0          0.0     0.000000  ...   \n",
       "\n",
       "   Component 94  Component 95  Component 96  Component 97  Component 98  \\\n",
       "0           0.0           0.0           0.0           0.0      0.000000   \n",
       "1           0.0           0.0           0.0           0.0      0.000000   \n",
       "2           0.0           0.0           0.0           0.0      0.000000   \n",
       "3           0.0           0.0           0.0           0.0      0.046971   \n",
       "4           0.0           0.0           0.0           0.0      0.000000   \n",
       "\n",
       "   Component 99  id                                           headline  \\\n",
       "0           0.0   0  Over 4 Million Americans Roll Up Sleeves For O...   \n",
       "1           0.0   1  American Airlines Flyer Charged, Banned For Li...   \n",
       "2           0.0   2  23 Of The Funniest Tweets About Cats And Dogs ...   \n",
       "3           0.0   3  The Funniest Tweets From Parents This Week (Se...   \n",
       "4           0.0   4  Woman Who Called Cops On Black Bird-Watcher Lo...   \n",
       "\n",
       "        date   category  \n",
       "0 2022-09-23  U.S. NEWS  \n",
       "1 2022-09-23  U.S. NEWS  \n",
       "2 2022-09-23     COMEDY  \n",
       "3 2022-09-23  PARENTING  \n",
       "4 2022-09-22  U.S. NEWS  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding the required columns from the original dataframe for creating a proper dataframe with labels\n",
    "\n",
    "nmf_features_df['headline'] = df['headline']\n",
    "nmf_features_df['date'] = df['date']\n",
    "nmf_features_df['category'] = df['category']\n",
    "\n",
    "nmf_features_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomly Generated Dataset for User Interaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_24128\\1639066178.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  user_df = user_df._append(temp_dict, ignore_index = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article 0</th>\n",
       "      <th>Article 1</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.73</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.72</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.33</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article 0  Article 1  user_id\n",
       "0       0.73       0.41      0.0\n",
       "1       0.35       0.72      1.0\n",
       "2       0.15       0.33      2.0\n",
       "3       0.19       0.01      3.0\n",
       "4       0.03       0.04      4.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "user_df = pd.DataFrame(columns = [f'Article {i}' for i in range(df.shape[0])])\n",
    "\n",
    "for i in range(50):\n",
    "    temp_dict = {'user_id': i}\n",
    "    for j in range(df.shape[0]):\n",
    "        clicks = random.randint(0,25)\n",
    "        like = random.choice([0,1]) if clicks >= 5 else 0\n",
    "        review = random.choice([0,1]) if clicks >= 10 else 0\n",
    "        temp_dict[f'Article {j}'] = clicks*0.01 + like*0.25 + review*0.5\n",
    "    user_df = user_df._append(temp_dict, ignore_index = True)\n",
    "\n",
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method to Combine Collaborative and Content Based Filtering methods for Final Prrediction\n",
    "\n",
    "In this we have to use the user interaction data for collaborative based filtering and the text article data for content based filtering.\n",
    "\n",
    "1) For less active users, we can use only content based filtering using a clustering model, and for which we can use the user interaction data for figuring out which articles the user has read, using a small threshold like 0.1 or 0.25.\n",
    "\n",
    "2) For more active users with a lot of interaction history (a user has to have more than 1000 or 5000 articles with scores >= 0.5), we can use a weighted result of both content based filtering and collaborative based filtering models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m15\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Apply Fuzzy C-Means\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m centres, initial_membership_matrix, final_membership_matrix, distances_matrix, obj_func_values, iteration_count, fpc \u001b[38;5;241m=\u001b[39m \u001b[43mfuzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcmeans\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcleaned_and_extracted_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Get the hard cluster membership for each data point\u001b[39;00m\n\u001b[0;32m     18\u001b[0m cluster_membership \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(final_membership_matrix, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\skfuzzy\\cluster\\_cmeans.py:173\u001b[0m, in \u001b[0;36mcmeans\u001b[1;34m(data, c, m, error, maxiter, metric, init, seed)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m p \u001b[38;5;241m<\u001b[39m maxiter:\n\u001b[0;32m    172\u001b[0m     u2 \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m--> 173\u001b[0m     [cntr, u, Jjm, d] \u001b[38;5;241m=\u001b[39m \u001b[43m_cmeans0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     jm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack((jm, Jjm))\n\u001b[0;32m    175\u001b[0m     p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\skfuzzy\\cluster\\_cmeans.py:27\u001b[0m, in \u001b[0;36m_cmeans0\u001b[1;34m(data, u_old, c, m, metric)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate cluster centers\u001b[39;00m\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m---> 27\u001b[0m cntr \u001b[38;5;241m=\u001b[39m \u001b[43mum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_2d(um\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     29\u001b[0m d \u001b[38;5;241m=\u001b[39m _distance(data, cntr, metric)\n\u001b[0;32m     30\u001b[0m d \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfmax(d, np\u001b[38;5;241m.\u001b[39mfinfo(np\u001b[38;5;241m.\u001b[39mfloat64)\u001b[38;5;241m.\u001b[39meps)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "cleaned_and_extracted_df = pd.read_csv(\"ML Datasets/ML Dataset 1/combined_cleaned_and_extracted.csv\")\n",
    "\n",
    "# Number of clusters\n",
    "n_clusters = 15\n",
    "\n",
    "# Apply Fuzzy C-Means\n",
    "centres, initial_membership_matrix, final_membership_matrix, distances_matrix, obj_func_values, iteration_count, fpc = fuzz.cluster.cmeans(\n",
    "    cleaned_and_extracted_df, n_clusters, 2, error=1e-5, maxiter=100, init=None)\n",
    "\n",
    "# Get the hard cluster membership for each data point\n",
    "cluster_membership = np.argmax(final_membership_matrix, axis=0)\n",
    "\n",
    "# Apply t-SNE to reduce to 3D\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "data_3d = tsne.fit_transform(cleaned_and_extracted_df.T)  # Transpose to match dimensions\n",
    "\n",
    "# Plotting the clusters in 3D\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each cluster with a different color\n",
    "for j in range(n_clusters):\n",
    "    ax.scatter(data_3d[cluster_membership == j, 0],\n",
    "               data_3d[cluster_membership == j, 1],\n",
    "               data_3d[cluster_membership == j, 2],\n",
    "               label=f'Cluster {j + 1}', s=50)\n",
    "\n",
    "# Plot the cluster centers\n",
    "center_3d = tsne.fit_transform(centres)  # Cluster centers in 3D\n",
    "ax.scatter(center_3d[:, 0], center_3d[:, 1], center_3d[:, 2],\n",
    "           s=300, marker='X', c='black', label='Centers')\n",
    "\n",
    "# Customize plot\n",
    "ax.set_title('3D t-SNE of Fuzzy C-Means Clustering')\n",
    "ax.set_xlabel('t-SNE Dim 1')\n",
    "ax.set_ylabel('t-SNE Dim 2')\n",
    "ax.set_zlabel('t-SNE Dim 3')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
